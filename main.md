课题背景研究
从半导体的发展开始，计算机越来越成为人们生活中不可离开的一部分。而因为互联网的发展，信息呈现出爆炸式增长，每天都有成千上万的网站上线，全世界每一个人都能够为互联网贡献自己的力量。但是人们获取知识的能力是有限的，而搜索引擎的出现让人们有机会发现更大的世界，它成了用户了解互联网的第一入口，而超大规模的数据给搜索引擎的的索引和搜索机制带来了更大的挑战，如今衡量搜索引擎性能好坏有三个指标[1]: “首先，原始网页数目的多少，这主要与搜索引擎爬虫的性能和规模有关，原始网页数目多，原始网页库规模大才会满足用户丰富多彩的需求,搜索引擎才会有市场;其次,搜索引擎的性能,主要体现在两方面,一是实时性,爬虫必须在较短的时间内更新原始网页库中最新数据。二是实效性,搜索引擎必须在较短的时间内返回用户提交搜索字段对应的结果集合;最后是搜索的效果,搜索引擎需要能够采用比较优秀的检索算法和排序算法,将用户想要的结果返回。”
	面对海量的数据，众多检索请求和检索时间的苛刻要求，搜索引擎中的每一环都对效率有着极高的要求。网络爬虫更是重中之重。网络爬虫系统的功能是下载网页数据，为搜索引擎系统提供数据来源，网页中除了包含一些文字数据，还包括一些超链接信息方便用户跳转。
	传统的网络爬虫采用的一台服务器来进行抓取操作，这就对服务器的硬件性能和稳定性都有很高的要求，而当所抓取的数据越来越多时，这种集中式的处理方式已经无法满足于当前互联网数据增长的需求。因此，采用多台机器进行分布式计算处理的爬虫应运而生。

论文的主要工作及贡献
本文在首先介绍了网络爬虫的基本原理和一些网络爬虫相关的经典算法，然后调研使用了了一些开源的网络爬虫框架并且在此基础之上，查阅了分布式技术的相关资料。从而设计出一个通用的满足高可靠性，高配置性，分布式抓取，运行稳定的框架。

论文的组织结构
第一章 绪论 简单介绍所选课题的背景及研究意义，另外包含了本文的主要工作以及研究内容并给出全文结构
第二章 介绍网络爬虫的基本原理和本程序中所使用到得一些技术
第三章 介绍分布式网络爬虫的整体架构和工作设计
第四章 介绍各个模块的具体实现 主要包括：队列模块 过滤器模块 robot模块 通信模块 抓取模块  页面存储模块等
第五章 介绍本爬虫运行所需条件配置，通过实验对本爬虫进行测试
第六章 总结和展望








第二章 网络爬虫的基本原理

网络爬虫的基本概念
搜索引擎是一种按照一定设计者给定的顺序在网络中穿梭并且自动收集有用的信息进行分类整理，并按照用户输入的请求将结果提交给用户的系统。搜索引擎主要由三大部分组成：信息采集模块（网络爬虫模块）、索引处理模块以及用户查询模块。
网络爬虫模块通过程序进行HTTP请求来模拟用户上网的方式来获取页面，同时将页面中的包含的url进行解析，接下来通过程序依次访问这些url。索引处理模块对这些页面进行预处理操作，通过提取关键词，分词等操作来建立索引，并且存入数据库。用户查询模块对用户的输入进行分析，并且从按照一定的排序方式数据库中提取出相关的信息。
网络爬虫负责从互联网中高效的抓取信息。这些信息有可能是简单地Web文本，也有可能是多媒体信息，比如图片等等。网络爬虫抓取的内容是否丰富，速度是否高效直接影响了网络爬虫的效果。基本网络爬虫的流程如下：1.从初始url集合中选出一个url发起http请求，下载所对应的页面；2. 解析该页面，从该页面中提取url集合 3. 如果url集合中的元素不在初始url集合中，并且还没有访问过，将这些url加入到初始url集合中；4.重复1，2过程直到满足停止条件为止。
互联网可以看做一个超级大的“图”，而每个页面可以看做是一个“节点”。页面中的链接可以看做是图的“有向边”。所以网络爬虫抓取的过程实质上就是一个图遍历算法的过程。

网络爬虫的功能特性

分布式：爬虫应该能在多台机器上运行来应对海量数据的问题。
可伸缩性：爬虫架构应该能够通过增加额外的机器和带宽来提高抓取速度
性能和有效性：爬虫系统必须有效地使用各种资源，例如：处理器、存储空间和网络带宽
质量：鉴于互联网的发展速度，大部分网页都不可能及时出现在用户查询中，所以爬虫应该首先抓取有用的网页。
新鲜性：在许多应用中，爬虫应该持续运行而不是只遍历一次
更新：因为网页会经常更新。例如论坛网站会经常有回帖。爬虫应该取得已经获得的页面的新的拷贝。
可扩展性：为了能够支持新的数据格式和抓取协议，爬虫应该设计成模块化的形式，保证了模块内部的健壮性的同时也能够方便的进行扩展。
礼貌性：爬虫应该对所爬取的服务器产生少量的流量压力，避免因短时间爬虫的抓取过程导致流量过高对该服务器的正常使用产生影响，导致对方服务器瘫痪或者爬虫程序被对方所屏蔽。所以建议在爬虫抓取的中间间隔一小段时间。同事爬虫还应该遵循对方服务器的拒绝蜘蛛协议（REP），有礼貌地抓取信息。
健壮性：一方面，网络上有很多含有蜘蛛陷阱的服务器，这些服务器会误导爬虫陷于某些区域内进行无穷无尽的页面爬取；另一方面，网络页面信息错综复杂，存有很多不规则的HTML代码页面，这些都是网络爬虫本身无法预知的，在解析和处理这些页面时，需要仔细考虑各种异常情况，防止爬虫程序出错或崩溃。所以，无论是蜘蛛陷阱，还是网站开发人员不小心犯下的错误，爬虫的设计都必须能够灵活处理这些陷阱爬虫，同时爬虫内部本身也要保持健壮性。

拒绝蜘蛛协议
拒绝蜘蛛协议通过在网站根目录下放置名为robots.txt（统一小写）的ASCII编码的文本文件来告诉网络搜索引擎的爬虫此网站的哪些内容是不应被搜索引擎的爬虫获取的，哪些是可以被获取的。因为一些系统的URL是大小写敏感的，所以robots.txt的文件名应统一为小写，并且放在网站的根目录下。如果想单独定义搜索引擎访问子目录的行为，可以将自定的设置合并到根目录下的robots.txt或者在该html头部定义：
`<meta name=”robots” content=”noindex,nofollow” />`
这个协议只是一个约定俗成的标准，大部分搜索引擎会遵守这一规范。通常搜索引擎会识别这个元数据，不索引这个页面。以及这个页面的链出页面。

robots.txt包含的基本规则如下：
1、User-agent: 该项的值用于描述搜索引擎robot的名字。
2、Disallow: 该项的值用于描述不希望被访问的一组URL，这个值可以是一条完整的路径，也可以是路径的非空前缀，以Disallow项的值开头的URL不会被 robot访问。
3、Allow: 该项的值用于描述希望被访问的一组URL，与Disallow项相似，这个值可以是一条完整的路径，也可以是路径的前缀，以Allow项的值开头的URL 是允许robot访问的。
4、Crawl-delay: 设置为整数值，表示在对本主机服务器上次访问结束后需要等待多长时间才可以进行下一次的访问请求。 (网络爬虫的礼貌策略)
5、#: 一些robots.txt 会有注释，#后面都是注释内容，处理时需要过滤掉。
6、通配符使用："$"匹配行结束符，"*"匹配0或多个任意字符。
网络爬虫的基本架构
[图 网络爬虫的架构]

待采集队列
	包含爬虫当前待抓取的URL（对于持续更新抓取的爬虫，以前已经抓取过的URL可能会回到队列中重新抓取）。爬虫队列是网络爬虫的关键。爬虫队列是用来保存URL的队列的数据结构。在大型爬虫应用中，构建通用的、可以拓展的爬虫队列非常重要。而当数据量非常大时，使用内存的链表或者队列来存储显然不够，因此，需要考虑如下方面：
能够存储海量数据，当数据超过内存限制时，能够把它固化在硬盘上
存取速度都非常快
DNS解析模块
	每个服务器都是有一个固定的IP地址，由四个整数以点为间隔组成大小为4个字节。为了方便用户使用，用将域名与IP相互映射到一个分布式的数据库,用户只需要访问域名，通过DNS解析服务就可以访问到IP所在的服务器。网络爬虫中的DNS解析模块将URL中的主机名进行DNS解析，获取目标主机的IP地址。从而决定从集群中的哪台机器进行爬取。由于域名服务的分布式的特点，DNS可能需要多次请求转发，并在互联网上往返，需要几秒有时甚至更长的时间解析出IP地址，这个过程是网络爬虫的瓶颈之一。
抓取模块
	抓取模块通过制定的URL地址，到目标服务器上获取网络资源。是于互联网交互最频繁、占用网络带宽最多的功能模块。网络资源可以为html本文或者其他多媒体文件，网络爬虫发送HTTP请求到目标服务器后，根据服务器响应的服务器编码识别此次请求的结果。
解析模块提取文本和网页的链接集合
	将抓取模块抓取到的文档进行解析，抽取出文本和URL集合，并且对URL进行一些处理之后，添加到待抓取队列。
重复消除模块
从第四步中产生的URL链接中筛选出最近没有下载过的或者已经在队列中的
URL过滤模块
	用户过滤特定的URL，需要支持拒绝蜘蛛协议的过滤要求，并应该具备良好的可拓展性
数据存储模块
包含存储页面文本页面数据库以及存储URL文本的URL数据库

分布式网络爬虫
随着互联网技术的发展以及风起云涌的云计算浪潮，爬虫技术也逐渐向着分布式的方向发展。虽然单机网络爬虫结构简单、方便维护、易于控制，但由于其硬件配置有限，无法完成大规模的分布式任务，而分布式技术不仅可以解决IIT运营成本的问题，还可以解决爬虫的效率问题。
第三章 分布式网络爬虫系统架构设计
分布式爬虫系统架构设计
常见的分布式网络爬虫系统架构主要分为三种：主从模式(master-slave)，点对点模式(Peer to Peer )和混合模式(Mixed)
	
一致性哈希算法的应用
哈希（hash）算法是一个广为应用的算法，用来将任意长度的二进制值映射为较短的固定长度的二进制值
爬虫节点间通信机制设计
在常规的网络通信中一般使用TCP/UDP进行网络传输，TCP（Transmission Control Protocol，传输控制协议）是基于连接的协议，也就是说，在正式收发数据前，必须和对方建立可靠的连接 ；而UDP（User Data Protocol，用户数据报协议）是与TCP相对应的协议。它是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发送过去，所以并不能保证传输的准确性。在爬虫中，由于各客户端必须接收到毫无差错的信息，所以我们使用TCP协议的应用层HTTP协议作为爬虫节点之间的通信协议。而在通信格式上，我们使用JSON来作为数据交换的格式。JSON是一种轻量级的网络传输格式，易于各种语言的生生与解析。
#### JSON格式简介
JSON（JavaScript Object Notation）是一种由道格拉斯·克罗克福特构想设计、轻量级的数据交换语言，以文字为基础，且易于让人阅读。尽管JSON是Javascript的一个子集，但JSON是独立于语言的文本格式，并且采用了类似于C语言家族的一些习惯。

JSON用于描述数据结构，有以下形式存在。

对象（object）：一个对象以“{”开始，并以“}”结束。一个对象包含一系列非排序的名称／值对，每个名称／值对之间使用“,”分区。
名称／值（collection）：名称和值之间使用“：”隔开，一般的形式是：
{name:value}
一个名称是一个字符串； 一个值可以是一个字符串，一个数值，一个对象，一个布尔值，一个有序列表，或者一个null值。

值的有序列表（Array）：一个或者多个值用“,”分区后，使用“[”，“]”括起来就形成了这样的列表，形如：
[collection, collection]
字符串：以""括起来的一串字符。
数值：一系列0-9的数字组合，可以为负数或者小数。还可以用“e”或者“E”表示为指数形式。
布尔值：表示为true或者false。在很多语言中它被解释为数组。
所以JSON在同样基于这些结构的编程语言之间交换成为可能。
【json图】

## 爬虫通信
爬虫客户端与主机端之间的通信模块模块由两部分组成：
主机端主要负责向不同的客户端分发URL队列，而客户端的接收机接收主机发送过来的URL数据包。客户端节点接收到URL之后，对URL队列中的数据进行合法化检验、去重、过滤等操作，将合法的URL添加进入客户端队列，当页面解析模块抽取出新的URL时，客户端分发器将属于本节点的URL重新进行合法化检验，去重放入合法化客户端队列。将不属于本节点的URL以及本节点的一些状态数据打包成JSON格式提交到爬虫的主机端，主机节点的接收器将客户端发送过来的JSON数据进行反序列化操作，通过主机端的DNS解析模块后，加入到主机端维护的一个客户端和URL队列之间的映射表。为了减少网络通信的次数，可以采取定时分发的通信机制。工作流程如图：


##第四章 分布式网络爬虫关键模块的研究与设计

#### 主机端待采集队列
##### URL数据结构设计
统一资源定位符（或称统一资源定位器/定位地址、URL地址等[1]，英语：Uniform / Universal Resource Locator，常缩写为URL），有时也被俗称为网页地址（网址）。如同在网络上的门牌，是因特网上标准的资源的地址（Address）。它最初是由蒂姆·伯纳斯-李发明用来作为万维网的地址。现在它已经被万维网联盟编制为因特网标准RFC 1738。

在因特网的历史上，统一资源定位符的发明是一个非常基础的步骤。统一资源定位符的语法是一般的，可扩展的，它使用ASCII代码的一部分来表示因特网的地址。统一资源定位符的开始，一般会标志着一个计算机网络所使用的网络协议。

统一资源定位符的标准格式如下：

协议类型://服务器地址（必要时需加上端口号）/路径/文件名

URL模块即是整个爬虫系统的核心对象，也是URL待采集队列的主要元素，它贯穿使用在整个爬虫系统之中。本文中URL的数据结构设计如下

基本属性| 数据类型|含义|值
-----|------|-------|------
HOST| 字符串|主机名host|"cs.xidian.edu.cn"
SEARCH|字符串|url在主机中的路径| "/news/undergraduate/1616.html"
PORT|整型|主机端口号，HTTP请求默认为80|80
PRIORITY|整型|URL优先级属性，权值越高，越优先爬取|20
DEPTH|整型|URL的爬虫深度|5
IP|字符串|DNS解析后对应的IP地址|"202.117.112.35"

当一个URL想要进入队列时，根据它的host和depth以及X设置优先级。优先级高的URL将会优先被取出队列进行分配
对于队列的存储我们使用Redis来进行存储
 Redis是一个由Salvatore Sanfilippo写的开源NoSQL存储系统。Redis提供了一些丰富的数据结构，包括 lists, sets, ordered sets 以及 hashes，Redis当然还包括了对这些数据结构的丰富操作。
Redis有以下优点：
* 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。
* 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。
* 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。
* 丰富的特性 – Redis还支持 publish/subscribe, 通知, key过期等等特性。
Redis不支持自定义数据库名字，每个数据库以编号命名，开发者必须自己记录哪些数据库存储了哪些数据。Redis也不支持为每个数据库设置不同的密码，所以一个客户端要么可以访问全部数据库，要么连一个数据库也没权限访问。
Redis的多个数据库之间不是完全隔离的，如FLUSHALL命令可以清空一个Redis实例中所有数据库中的数据。在同一个Redis实例的不同数据库存储相同应用的数据不同应用的数据存放到不同的Redis实例中。
  在我们的分布式爬虫中，我们使用redis来实现一个存储待抓取的URL的队列,因为在redis中出队列操作和入队列操作都是o(1)的花费列表类型键可以用来实现队列，并支持阻塞时读取，可以很容易地实现一个高性能的优先级队列。

## 页面抓取模块

页面抓取模块是整个爬虫模块和外界网络联系的接口。通过HTTP协议与网络服务器之间进行网络通信
抓取模块的工作流程：
1. 对URL对应的主机名进行DNS解析，如果DNS之前被解析过，则会通过DNS缓存读取，得到主机所对应的IP地址。
2. 爬虫通过IP地址和端口号对目标服务器发送HTTP GET请求
3. 服务器接收到爬虫的请求之后，会返回一个包含HTTP响应码的响应头和附加内容，爬虫对收到的HTTP状态码进行解析。当响应码为2XX的时候，证明请求的资源文件成功，爬虫需要将返回的状态头和附加内容（此时为整个文档）交给解析模块和存储模块做进一步处理。当响应码为3XX时，表明请求的资源文件已经转移到头信息中Location指定的位置，可以根据配置设置爬虫是否需要根据此重定向进行抓取
4. 为了保障爬虫不会陷入爬虫陷阱，即通过几个网页之中无限循环的跳转，爬虫一般会设置最大的重定向次数。当响应码为其他值时，将其存储在特定的文件之中，供日志查看和分析。
【HTTP响应码table】

爬虫在此过程需要和网络进行通信，有些服务器可能延时比较高或者页面比较大，这有可能是一个很耗时的过程。因此我们需要在爬虫抓取的时候设置一个最大值的超时值，如果超过这个值，爬虫将会认为这个过程中的页面是不可达的，将会自动放弃此次任务，并将这次抓取过程存入日志文件中。

#### 页面解析模块
在整个爬虫系统中，页面解析模块负责对页面内容进行解析，抽取出HTML文档中的URL资源（只包含文档的页面链接，不包含CSS，JS文件等）链接，并且对抽出的URL做一些合法性的判断，去掉重复的URL，过滤掉已经抓取过得URL，最后将其中属于本节点的URL放入到本节点的队列中，不符合本节点的URL添加到主机端的队列中。
爬虫抓取的文档基本都是HTML/XML格式的文本，有一定的规范要求，故提取链接地址并不是特别复杂。
需要在URL提取中注意URL提取的准确性，而且要保证算法具有比较高的效率。网页中匹配URL主要是匹配`<body>`标签中的`<a>`标签中的`href`属性和`<iframe>`标签中的`src`属性，这两个属性包含了文档指向其他文档的超链接。
提取到必要的统一资源地址信息之后，对于相对地址要转换为绝对地址，并将不满足爬虫需求和地址中的重复的锚节点过滤掉。之后对于地址评判优先级并且更新爬虫的深度。将爬虫深度大于0并且小于配置深度的URL添加到URL采集队列中。循环以上流程，直到提取出需要抓取的所有地址。

####  URL去重模块

一般来说，我们判断一个元素是否在一个集合中，最直接的方法是讲集合中的全部元素存在数组中，遇到一个新元素，将它与集合中的元素进行直接比较。HASH表正好满足这种需求，而且它快速又准确，但是有个显著的缺点就是浪费空间。当集合比较少时，这个问题并不是很明显，但是当集合巨大时，尤其要适应于分布式系统时，URL队列可能非常巨大，抓取过得数据可能非常多，哈希表的低效率就显现出来了。在这里我们使用一种叫做布隆过滤器（Bloom Filter）的数学工具，它只需要哈希表1/8到1/4的大小就能解决同样地问题。
布隆过滤器实际上是一个很长的二进制向量和一系列随机映射函数组成。默认将二进制所有位数置为0，当一个地址被加入集合时，我们通过K个随机散列函数（HASH）将这个元素映射成这个位数组中的K个点，然后将这K个点置为1.检查时我们只需要看这些点是否都为1，就能判断集合中有没有这个地址。如果这些点有任何一个为0，则检查元素一定不再，如果都是1，则被检查元素很可能存在，之所以很可能，是因为随机散列函数有一定情况下出现误判，两个不同的URL产生相同的随机值。但是随着元素数量的增加，误算率会逐渐减少。当我们为每个URL分配2个字节就可以达到千分之几的冲突。比较保守的实现是，为每个URL分配4个字节，项目和位数比是1：32，误判率是0.00000021167340 对于5000万数量级的URL，布隆过滤器只占用200MB，而且重排速度超级快，不到两分钟。

## 数据存储模块

对于数据的存储，可能首选的方案是关系型数据库。关系型数据库确实是存储文档的理想模型，而且关系型数据库技术成熟，有固定的表结构，而且能够自动建立索引，方便查找和更新数据，而且大多关系型数据库系统都被设计为网络服务器上运行。然而很多爬虫系统很少的使用关系型数据库进行存储，因为关系型数据库处于高速运行时，会对数据库进行频繁的访问。所以针对以下特点我们使用非关系型数据库（NoSQL）来进行数据存储，其中我们使用MongoDB来存储海量数据。MongoDB（来自于英文单词“Humongous”，中文含义为“庞大”）是可以应用于各种规模的企业、各个行业以及各类应用程序的开源数据库。作为一个适用于敏捷开发的数据库，MongoDB的数据模式可以随着应用程序的发展而灵活地更新。与此同时，它也为开发人员 提供了传统数据库的功能：二级索引，完整的查询系统以及严格一致性等等。 MongoDB能够使企业更加具有敏捷性和可扩展性，各种规模的企业都可以通过使用MongoDB来创建新的应用，提高与客户之间的工作效率，加快产品上市时间，以及降低企业成本。
MongoDB是专为可扩展性，高性能和高可用性而设计的数据库。它可以从单服务器部署扩展到大型、复杂的多数据中心架构。利用内存计算的优势，MongoDB能够提供高性能的数据读写操作。 MongoDB的本地复制和自动故障转移功能使您的应用程序具有企业级的可靠性和操作灵活性。 
在本系统中，我们使用Mongodb来存储得到的网页的内容等内容

【数据存储结构图】


## 第五章 实验效果及分析

