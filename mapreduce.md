# MapReduce: 在大集群中简单的数据处理

标签（空格分隔）： 未分类

---

# 绪论

MapReduce是一个处理和生成大规模数据集的编程模型。用户定义一个`map`函数来处理`key/value`对，一个`reduce`函数来归并拥有相同`key`的所有中间值.许多现实世界中的任务表现出了这种方法，在这片论文中有所列举。
使用这种函数式编程的方法写出的程序在大规模集群中自动并行化，这个实时系统关注输入数据的分割，在一系列集群中进行程序调度执行，管理集群之间的数据通信。允许没有任何并行和分布式系统经验的程序员简单地去处理大规模的资源。
我们的MapReduce的实现运行在规模可以灵活调整的由普通机器组成的机群上：一个典型的MapReduce计算过程处理几千台机器上的以TB计算的数据。程序员发现这个系统很容易使用：每天的google集群上都有成千上百的程序已经使用并且高达几千的MapReduce程序

# 1.简介
在过去的五年中，作者和google的其他同事已经实现了几百个特定功能的计算模型来处理大规模数据，例如抓取文档，网络请求日志等，还有计算各种衍生数据，例如倒排索引,Web文档的图结构的各种表示,每个主机中文档抓取的总结，一天之中最频繁访问的链接等。绝大部分计算是直观的。但是，输入数据通常是很大的而且为了在一个合情合理的时间内完成计算，通常将计算分布在成百上千的机器中。怎样并行计算,分发数据,处理错误,所有这些问题综合在一起,使得原本很简介的计算,因为要大量的复杂代码来处理这些问题,而变得让人难以处理. 
作为对这个复杂性的回应，我们设计了一个允许我们去表达简单的计算但是将复杂的并行细节，容错处理，数据分布和负载均衡屏蔽掉的新的抽象概念。我们的抽象概念的灵感来自于最先出现在`LISP`和其他函数式编程语言中的中的`map`和`reduce`。我们意识到大多数我们的计算模型涉及到应用一个`map`操作来对每一个逻辑“记录”在我们的输入中来确认计算一系列的中间的key/value对，然后使用`reduce`为了精确地结合衍生的数据，对所有的有相同key的数据进行操作。我们使用用户特定的函数式模型`map`和`reduce`允许我们轻便的进行并行大规模计算和使用再次执行作为初级机制来实现容错.
这个工作主要贡献是使用简单并且强大的接口来自动进行并行和大规模分布式计算，通过结合这个接口能够在大量的普通PC上实现高性能的运算。
第二部分描述了基本的编程模型，并且给定了一些例子。第三部分描述了MapReduce得一个应用，定义了基于集群的计算环境。第四章描述了一些我们发现非常有用的对这一编程模型的改进，第五部分包含大量的任务中性能的衡量。第六部分探索了在Google内部使用MapReduce作为基础来重写我们的索引系统产品。第七章讨论了相关内容和未来的工作。
# 2。 编程模型
计算模型接受一系列的key/value对，生成一系列key/value对。MapReduce库的使用者通过`Map`和`Reduce`函数来表达计算。
`Map`，由使用者定义。接受一个key/value列表，生成一系列的**中间值** key/value对。MapReduce库 集合所有具有相同中间值 `key I`的序对并且将他们传递给Reduce函数。
`Reduce`也是由使用者定义，接受一个中间值`key I`和一系列它的值。reduce函数将这些值合并起来并且形成一个比较小的value集，通常每次reduce只会产生0或者1。这些中间值将会通过一个迭代器应用到用户的reduce函数中。这允许我们处理在内存中过于多的列表值。
### 2.1 举例
考虑这个问题:计算在一个大的文档集合中每个词出现的次数.用户将写和下面类似的伪代码:
```java
map(String key,String value):
    //key : 文档名
    //value: 文档内容
    for each word w in value:
        EmitIntermediate(w,"1");
reduce(String key,Iterator values):
    //key:单词
    //values:计数列表
    int result = 0;
    for each v in values:
        result += ParseInt(v);
    Emit(AsString(result));
```
`map`函数产生每个词和这个词的出现次数(在这个简单地例子里就是1)`reduce`函数将特定词的所有次数加在一起
另外，用户用输入输出文件的名字和可选的调节参数来填充一个mapreduce规范对象。然后用户引用MapReduce函数，向它传递特定的对象。用户的代码和MapReduce的库（C++实现）相互连接。附录A包含这个例子的一个完整的程序。

### 2.2 类型
精工我们之前提供的伪代码采用字符串的输入输出，概念上来讲，map和reduce函数有关联的类型：
```
map (k1,v1)    -> list(k2,v2)
reduce (k2,list(v2)) ->list(v2)
```
例如，输入的key,value和输出的key,value的域不同.或者，中间值的key和value和输出的key和value来自相同的域。
我们的C++应用通过传递字符串来和用户定义的函数进行通信，并且将它留给了用户的代码在字符串和合适的类型中去转换。
### 更多的例子
这里有一些简单地有趣的程序能够解释MapReduce计算模型
**分布式Grep** map函数操做匹配到提供的模式的行。reduce函数是一个标记函数，仅仅复制并且应用到中间值作为输出
**计算URL连接的频繁度** map函数处理网页请求的日志并且输出`<URL,1>`然后reduce函数增加所有具有相同url的值在一起并且触发一个`<URL，total count>`序对。
**倒转网络链接图** map函数为每个url页面中的任何一个链接生成`<target,source>`序对，target是一个URL叫做目标,包含这个URL的页面叫做源.reduce函数根据给定的相关目标URLs连接所有的源URLs形成一个列表,产生`target,list(source)`对.
**每个主机的术语向量**一个术语向量用一个(词,频率)列表来概述出现在一个文档或一个文档集中的最重要的一些词.map函数为每一个输入文档产生一个(主机名,术语向量)对(主机名来自文档的URL).reduce函数接收给定主机的所有文档的术语向量.它把这些术语向量加在一起,丢弃低频的术语,然后产生一个最终的(主机名,术语向量)对.
[图片1]
**倒排索引** map函数分析每个文档,然后产生一个(词,文档号)对的序列.reduce函数接受一个给定词的所有对,排序相应的文档IDs,并且产生一个(词,文档ID列表)对.所有的输出对集形成一个简单的倒排索引.它可以简单的增加跟踪词位置的计算.
**分布式排序**:map函数从每个记录提取key,并且产生一个(key,record)对.reduce函数不改变任何的对.这个计算依赖分割工具(在4.1描述)和排序属性(在4.2描述).

### 3. 应用
MapReduce的接口有很多实现，正确的选择依赖于环境。例如，一个实现可能适合于小型的共享内存机器，另一个适用于大型的NUMA处理器，或者另一个for一个大型的网络机器的集合
这章描述一个基于目标环境的在google中广为使用的实现：用交换机连接的普通PC机的大机群，在我们的环境中：
1. 机器通常是多核处理器X86架构运行Linux的内存在2-4GB
2. 通用网络硬件使用100mb/s或者1gb/s的机器级别，但是平均小于全部带宽的一半
3. 一个集群包含成千上百的机器，因此机器失败是很普遍的
4. 直接连到每个机器上的廉价IDE硬盘。一个内部分布式文件系统来处理这些存在硬盘上的数据。这个数据系统使用冗余复制的方式在不可靠的的机器上保证可靠性和有效性.
5. 用户提交工作给调度系统。每个工作包含一系列的任务,每个工作被调度者映射到机群中一个可用的机器集上

### 3.1 执行总览
map调用通过自动分割输入数据成一个有M个split的集被分布到多台机器上.输入分割可能被并行的使用在不同的机器中。reduce也是分布式的，调用使用一个分割函数（例如：hash(key)mod R）通过分割中间值的key为R片。分片数目R和分片函数由用户来定义。
图1展示了我们的实现中整个MapReduce操作的流程。当用户程序调用MapReduce函数时，以下的行动序列发生（图一中的数字序列对应于下面列表中的数字）
1. 用户程序中的MapReduce库开始分割文件为M块，大小通常为16MB到64MB每块（通过配置参数由用户定义）。然后它开始在集群中执行此程序
2. 其中一个程序是特殊的--master，其他的程序的工作都是由master分配的。这里有M个map任务和R个reduce任务去分配，master挑选出其中空闲的机器进行map任务或者reduce任务。
3. 被分配了map任务的worker读取相关输入切分的内容。它从输入数据中解析key/value对，然后将它们传递给用户定义的Map函数。Map函数生成的中间值key/value对存在内存的缓存中。
4. 周期性的，缓存中的序对被写到硬盘中，被分割函数分割成R个区域。这些缓存在本地磁盘的位置被返回到master中，以便master分配reduce任务的时候使用
5. 当一个reduce机器被master通知这些位置时，它从map任务的本地磁盘中使用远程读取这些存取的数据。当一个reduce机器读完所有的中间值，它通过中间key进行排序，以便所有的具有相同key的值聚在一起。排序是必须的，因为许多不同keys的map有相同的reduce任务。如果中间值的数据在内存中太大，还需要一个外部排序.
6. reduce机器迭代排序过的中间数据，对每个独立的中间key值进行累加，它传递了key和相符合的中间值的集合给用户的Reduce程序。
7. 当所有的map任务和reduce任务都完成。master执行用户的程序，在此刻，管理者唤醒用户程序.在这个时候,在用户程序里的MapReduce调用返回到用户代码.

在所有成功进行后，mapreduce输出结果已经可以使用（每一个都是reduce的任务，文件名由用户来定义）。特别的，用户不需要合并这个R个输出文件到一个文件，他们通常把这些文件当做输入文件放到另外一个mapreduce程序中，或者用户使用其他的分布式应用，他们能够处理这些分块输入文件。

### 3.2 master的数据结构
master有许多数据结构。每个map任务和reduce任务，它存储任务的状态(空闲，处理中，完成)，和用户机的身份识别(对于非空闲的任务)
master就像一个管道,通过它,中间文件区域的位置从map任务传递到reduce任务.因此，对于每一项完成的任务,master存储地址和map过程生成的R中间值文件区域和大小。当map任务完成时，更新这些地址和大小信息，这些信息被逐步增加的传递给那些正在工作的reduce任务
### 3.3 容错处理
MapReduce库被设计为去帮助分布式的运行超大规模的数据，所以这个库必须要能很好的处理机器故障.
**worker机器故障**
master周期性的ping每个worker.如果master在一个确定的时间段内没有收到worker返回的信息,那么它将把这个worker标记成失效.因为每一个由这个失效的worker完成的map任务被重新设置成它初始的空闲状态,所以它可以被安排给其他的worker.同样的,每一个在失败的worker上正在运行的map或reduce任务,也被重新设置成空闲状态,并且将被重新调度.
在一个失败机器上已经完成的map任务将被再次执行,因为它的输出存储在它的磁盘上,所以不可访问.已经完成的reduce任务将不会再次执行,因为它的输出存储在全局文件系统中.
当一个map任务首先被worker A执行之后,又被B执行了(因为A失效了),重新执行这个情况被通知给所有执行reduce任务的worker.任何还没有从A读数据的reduce任务将从worker B读取数据.
MapReduce可以处理大规模worker失败的情况.例如,在一个MapReduce操作期间,在正在运行的机群上进行网络维护引起80台机器在几分钟内不可访问了,MapReduce master只是简单的再次执行已经被不可访问的worker完成的工作,继续执行,最终完成这个MapReduce操作.

**master失败**
可以很容易的让管理者周期的写入上面描述的数据结构的checkpoints.如果这个master任务失效了,可以从上次最后一个checkpoint开始启动另一个master进程.然而,因为只有一个master,所以它的失败是比较麻烦的,因此我们现在的实现是,如果master失败,就中止MapReduce计算.客户可以检查这个状态,并且可以根据需要重新执行MapReduce操作

**在错误面前的处理机制**
当用户提供的map和reduce操作对它的输出值是确定的函数时,我们的分布式实现产生,和全部程序没有错误的顺序执行一样,相同的输出.
我们依赖对map和reduce任务的输出进行原子提交来完成这个性质.每个工作中的任务把它的输出写到私有临时文件中.一个reduce任务产生一个这样的文件,而一个map任务产生R个这样的文件(一个reduce任务对应一个文件).当一个map任务完成的时候,worker发送一个消息给master,在这个消息中包含这R个临时文件的名字.如果master从一个已经完成的map任务再次收到一个完成的消息,它将忽略这个消息.否则,它在master的数据结构里记录这R个文件的名字.
当一个reduce任务完成的时候,这个reduce worker原子的把临时文件重命名成最终的输出文件.如果相同的reduce任务在多个机器上执行,多个重命名调用将被执行,并产生相同的输出文件.我们依赖由底层文件系统提供的原子重命名操作来保证,最终的文件系统状态仅仅包含一个reduce任务产生的数据.
我们的map和reduce操作大部分都是确定的,并且我们的处理机制等价于一个顺序的执行的这个事实,使得程序员可以很容易的理解程序的行为.当map或/和reduce操作是不确定的时候,我们提供虽然比较弱但是合理的处理机制.当在一个非确定操作的前面,一个reduce任务R1的输出等价于一个非确定顺序程序执行产生的输出.然而,一个不同的reduce任务R2的输出也许符合一个不同的非确定顺序程序执行产生的输出.
考虑map任务M和reduce任务R1,R2的情况.我们设定e(Ri)为已经提交的Ri的执行(有且仅有一个这样的执行).这个比较弱的语义出现,因为e(R1)也许已经读取了由M的执行产生的输出,而e(R2)也许已经读取了由M的不同执行产生的输出.
 
### 3.4存储位置
在我们的计算机环境里,网络带宽是一个相当缺乏的资源.我们利用把输入数据(由GFS管理)存储在机器的本地磁盘上来保存网络带宽.GFS把每个文件分成64MB的一些块,然后每个块的几个拷贝存储在不同的机器上(一般是3个拷贝).MapReduce的master考虑输入文件的位置信息,并且努力在一个包含相关输入数据的机器上安排一个map任务.如果这样做失败了,它尝试在那个任务的输入数据的附近安排一个map任务(例如,分配到一个和包含输入数据块在一个switch里的worker机器上执行).当运行巨大的MapReduce操作在一个机群中的一部分机器上的时候,大部分输入数据在本地被读取,从而不消耗网络带宽.
### 3.5任务粒度
象上面描述的那样,我们细分map阶段成M个片,reduce阶段成R个片.M和R应当比worker机器的数量大许多.每个worker执行许多不同的工作来提高动态负载均衡,也可以加速从一个worker失效中的恢复,这个机器上的许多已经完成的map任务可以被分配到所有其他的worker机器上.
在我们的实现里,M和R的范围是有大小限制的,因为master必须做O(M+R)次调度,并且保存O(M*R)个状态在内存中.(这个因素使用的内存是很少的,在O(M*R)个状态片里,大约每个map任务/reduce任务对使用一个字节的数据).
此外,R经常被用户限制,因为每一个reduce任务最终都是一个独立的输出文件.实际上,我们倾向于选择M,以便每一个单独的任务大概都是16到64MB的输入数据(以便上面描述的位置优化是最有效的),我们把R设置成我们希望使用的worker机器数量的小倍数.我们经常执行MapReduce计算,在M=200000,R=5000,使用2000台工作者机器的情况下.
### 3.6备用任务
一个落后者是延长MapReduce操作时间的原因之一:一个机器花费一个异乎寻常地的长时间来完成最后的一些map或reduce任务中的一个.有很多原因可能产生落后者.例如,一个有坏磁盘的机器经常发生可以纠正的错误,这样就使读性能从30MB/s降低到3MB/s.机群调度系统也许已经安排其他的任务在这个机器上,由于计算要使用CPU,内存,本地磁盘,网络带宽的原因,引起它执行MapReduce代码很慢.我们最近遇到的一个问题是,一个在机器初始化时的Bug引起处理器缓存的失效:在一个被影响的机器上的计算性能有上百倍的影响.
我们有一个一般的机制来减轻这个落后者的问题.当一个MapReduce操作将要完成的时候,master调度备用进程来执行那些剩下的还在执行的任务.无论是原来的还是备用的执行完成了,工作都被标记成完成.我们已经调整了这个机制,通常只会占用多几个百分点的机器资源.我们发现这可以显著的减少完成大规模MapReduce操作的时间.作为一个例子,将要在5.3描述的排序程序,在关闭掉备用任务的情况下,要比有备用任务的情况下多花44%的时间.

